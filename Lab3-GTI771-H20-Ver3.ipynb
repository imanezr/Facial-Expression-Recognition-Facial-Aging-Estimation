{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Lab3-GTI771-H20-Ver3.ipynb","provenance":[],"collapsed_sections":["MwbFvsHe1Rif","UwtXJc8h1RjX"],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"dt3S-OEr1Rgk"},"source":["# Laboratoire 3\n","# SVM, ensembles et apprentissage non-supervisé\n","## Classification, régression et régroupement (FER et FG-NET)\n","\n","### GTI771 - Apprentissage machine avancé\n","#### Département du génie logiciel et des technologies de l’information\n","\n","#### Version 1.0 mars 2020\n","#### <font color=blue> Version 2.0 - 22 mars 2020 </font>\n","#### <font color=magenta> Version 3.0 - 3 avril 2020 </font>\n","\n","##### Prof. Alessandro L. Koerich"]},{"cell_type":"markdown","metadata":{"id":"xjJoAlVf1Rgn"},"source":["| Étudiants             | Branavan Inthirnathan - INTB25099502                    |\n","|-----------------------|---------------------------------------------------------|\n","| Session               | HIV 2020                                            |\n","| Équipe                | 07                                                       |\n","| Numéro du laboratoire | 3                                                       |\n","| Professeur            | Prof. Alessandro L. Koerich                                               |\n","| Chargé de laboratoire |  Alessandro L. Koerich                                                     |\n","| Date                  | 04/19/20                                                    |"]},{"cell_type":"markdown","metadata":{"id":"ddhkx98B1Rgp"},"source":["# Introduction\n","\n","Ce troisième laboratoire porte sur l’utilisation des machines à vecteurs de support (SVM) et régresseurs à vecteurs de support (SVR), la combinaison de modèles d’apprentissage et le regroupement.\n","\n","Dans ce laboratoire, vous êtes amenés à utiliser de nouvelles approches à l’aide de ces algorithmes/approches afin de résoudre le problème de classification des émotions (FER dataset) et le problème de régression des âges (FG-NET dataset). De plus, dans ce laboratoire, vous êtes amené aussi à considérer les algorithmes d’apprentissage non supervisé.\n","\n","Vous devrez utiliser les primitives développées aux premiers laboratoires lorsque celles-ci ne sont pas redondantes. Vous devrez également étudier les hyperparamètres du modèle. Tout comme les deux premiers laboratoires de ce cours, vous réaliserez ce troisième travail avec la technologie Python3 conjointement avec la librairie d’apprentissage machine scikit-learn.\n","\n","Votre Jupyter notebook devra contenir les réponses aux questions. Il devra notamment avoir une analyse détaillée des résultats de classification obtenus par les différents modèles et leurs variations d’hyperparamètres.\n","\n","L’évaluation de ce laboratoire sera basée sur la qualité des modèles entraînés, la comparaison des performances réalisées par les différents modèles, les réponses aux questions dans cette notebook ainsi que l’organisation de votre code source (SVP, n’oubliez pas des commentaires dans le code!)."]},{"cell_type":"markdown","metadata":{"id":"P8aKHVC41Rgq"},"source":["# Objectifs\n","\n","Les principaux objectifs de ce laboratoire sont :\n","1. Produire un code source utilisant les technologies Python et des techniques d’apprentissage machine permettant de classifier des échantillons de données automatiquement;\n","2. Se familiariser avec les algorithmes SVM et SVR;\n","3. Analyser l’impact des hyperparamètres et des différents noyaux utilisés;\n","4. De construire un véritable système intelligent avec une combinaison de modèles (et/ou primitives);\n","5. Se familiariser avec les algorithmes d’apprentissage non supervisé;\n","6. Analyser et interpréter les sorties des algorithmes non supervisés;\n","7. Se familiariser avec le langage de programmation Python et les outils et librairies scientifiques d’apprentissage machine;"]},{"cell_type":"markdown","metadata":{"id":"EEia-qtu1Rgs"},"source":["* #### Partie 1: SVM - Classification et régression\n","* SVM lineaire\n","* SVM noyau polynomial\n","* SVM noyau RBF\n","<font color=blue>\n","* SVR lineaire ou SVR noyau polynomial ou SVR noyau RBF\n","</font>    \n","<br>\n","* #### Partie 2: Ensembles\n","<font color=blue>\n","* Bagging, Boosting / Adaboost\n","* Votation, Min, Max, Average, Product, Weighted Rules\n","</font>        \n","<br>\n","* #### Partie 3: Regroupement\n","<font color=magenta>\n","* k-Means\n","* Autre méthode de regroupement\n","</font>    "]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"SX_6zwJe1Rgu"},"source":["## Partie 0: Imports"]},{"cell_type":"markdown","metadata":{"id":"UWbi5BTJ1Rgv"},"source":["### (0a) Import de libraries"]},{"cell_type":"markdown","metadata":{"id":"smVJv6Xx1Rg0"},"source":["##### À faire:\n","1. Ajouter toutes les bibliothèques que vous avez utilisées pour compléter ce notebook dans une cellule avec une petite description."]},{"cell_type":"code","metadata":{"id":"MQFQoDuR1Rg2"},"source":["import numpy as np  # package for scientific computing with Python.\n","import matplotlib.pyplot as plt # 2D plotting library\n","from sklearn.preprocessing import MinMaxScaler #Normalizer\n","import os \n","import math\n","import csv\n","from sklearn import svm\n","from hypopt import GridSearch #hyperparametre optimization\n","from sklearn.tree import DecisionTreeClassifier #classifier 2b\n","from sklearn.ensemble import RandomForestClassifier #classifier 2b\n","from sklearn.neighbors import KNeighborsClassifier#classifier 2b\n","from sklearn.naive_bayes import GaussianNB #classifier 2b\n","from sklearn.ensemble import VotingClassifier #classifier 2b\n","from sklearn.model_selection import GridSearchCV #classifier 2b for grid search\n","from sklearn.metrics import accuracy_score\n","from joblib import dump, load #export les modèles\n","from hypopt import GridSearch #hyperparametre optimization\n","#import thundersvm\n","from sklearn import tree\n","###### Clusters ##########\n","import pandas as pd\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import DBSCAN\n","from sklearn.cluster import SpectralClustering \n","from sklearn.cluster import Birch\n","\n","from sklearn import metrics\n","from sklearn.datasets import make_blobs\n","from sklearn.metrics import silhouette_score\n","from sklearn.metrics.cluster import adjusted_rand_score\n","\n","# Evaluation metrics\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"26YjNj2n1Rg9"},"source":["### (0b) Charger le fichier de données\n","\n","##### À faire:\n","1. Ajouter le code pour lire les fichiers de données."]},{"cell_type":"code","metadata":{"id":"TvFBvpNJ12xy","executionInfo":{"status":"ok","timestamp":1587340113341,"user_tz":240,"elapsed":4032,"user":{"displayName":"Imane Zriaa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioDY4dLkpnQFW2LMOcW3EpM23co77w9n8Vs1QD=s64","userId":"18312604134454482575"}},"outputId":"5da17e22-1fc8-4b60-8e80-fbf1718f5b84","colab":{"base_uri":"https://localhost:8080/","height":100}},"source":["pip install thundersvm"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: thundersvm in /usr/local/lib/python3.6/dist-packages (0.3.12)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from thundersvm) (1.18.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from thundersvm) (1.4.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from thundersvm) (0.22.2.post1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->thundersvm) (0.14.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mVgeDhk-1Rg-"},"source":["# fix random seed for reproducibility\n","seed = 7\n","np.random.seed(seed)\n","# load data\n","ferData = np.loadtxt('fer2013.csv', delimiter=',', dtype=np.str)\n","Xtrain = np.ones((28709,2304),np.uint8)\n","for i in range(1, 28710):\n","\tXtrain[i-1]=ferData[i,1].split(\" \")\n","\n","ytrain=ferData[1:28710,0].astype(np.int)\n","Xval = np.ones((3589,2304),float)\n","for i in range(28710, 32299):\n","\tXval[i-28710]=ferData[i,1].split(\" \")\n","\n","yval=ferData[28710:32299,0].astype(np.int)\n","Xtest = np.ones((3589,2304),float)\n","for i in range(32299, 35888):\n","\tXtest[i-32299]=ferData[i,1].split(\" \")\n","\n","ytest=ferData[32299:,0].astype(np.int)\n","\n","# normalize\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","\n","Xtrain = scaler.fit_transform(Xtrain)\n","Xval   = scaler.fit_transform(Xval)\n","Xtest  = scaler.fit_transform(Xtest)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3rAv-UY31RhE"},"source":["# Partie 1: Machines à vecteurs de support (SVM)\n","## (FER et FG-NET dataset)\n","\n","Dans cette partie vous devez explorer les <b> différentes noyaux du SVM </b> disponibles dans LIBSVM, comme linéaire, polynomial et RBF.\n","\n","Vous devez comparer la performance de ces algorithmes pour les ensemble FER (classification) et FG-NET (régression)"]},{"cell_type":"markdown","metadata":{"id":"XyDfWPL41RhF"},"source":["## (1a) Créer et évaluer des modèles SVM de classification\n","### (FER dataset)\n","\n","[Scikit-learn LibSVM](https://scikit-learn.org/stable/modules/svm.html)"]},{"cell_type":"markdown","metadata":{"id":"HfxGh7C91RhH"},"source":["##### À faire:\n","1. Vecteurs de primitives << artisanales >> (handcrafted sans/avec réduction) du laboratoire 1. Choisir une seule représentation (p. ex. LBP après PCA). N’oubliez pas de normaliser les vecteurs (entrées) entre 0 et 1. \n","2. Vecteurs de primitives << deep >> (CNN features sans/avec réduction) du laboratoire 2. Choisir une seule représentation (p. ex. 1re couche entièrement connectée du CNN simple). Vous pouvez également appliquer un algorithme de réduction de la dimensionnalité pour réduire la dimension de ce vecteur. N’oubliez pas de normaliser les vecteurs (entrées) entre 0 et 1.\n","3. Pour chaque représentation (artisanale et deep):<br> \n","3a. Entraîner et optimiser (C) un SVM linéaire <br>\n","3b. Entraîner et optimiser (C et ordre) un SVM polynomial <br>\n","3c. Entraîner et optimiser (C et gamma) un SVM RBF (Gaussian) <br>\n","4. Analyser les résultats et présenter vos conclusions sur les modèles SVM.\n","\n","|  Primitive   | Noyau       |   Paramètres    |  % App | % Val  | % Tst  | \n","|--------------|-------------|-----------------|--------|--------|--------|\n","| LBP avec PCA | lineaire    | C=256            | 19.123 | 18.501 | 19.170       |\n","| LBP avec PCA | polynomial  | C=10,d=6         |  17.311 |16.968        | 17.414       |\n","| LBP avec PCA | RBF         | C=256, g=0.01   | 18.865 | 18.807 | 17.414 |\n","| CNN 5L 2FC     | lineaire    | C=10            | 25.37 | 16.912 |  17.024      |\n","| CNN 5L 2FC     | polynomial  | C=10,d=3         |  76.683 | 14.712       |  16.299      |\n","| CNN 5L 2FC     | RBF         | C=10,d=3,g=0.05   | 91.946 |16.188 | 18.278     |\n","\n","###Analyse des résultats###\n","\n","Avec l'utilisation de primitives des laboratoires précédant, soit le vecteur artisanal de LBP avec l'application de l'algorithme de transformation et le vecteur deep avec le CNN de 5 couches convolutifs et 2 couches entièrement connectées, dans les modèles SVM de classification. On peut observer selon les résultats des différents vecteurs dans les modèles SVM que la précision de ces primitives est sensiblement la même, en effet avec la primitive LBP/PCA, elle donne entre 17% et 19% pour  l'ensemble d'entrainement, de validation et test. Par la suite, avec la primitive CNN, elle donne entre 15% et 18% pour l'ensemble de validation et de test avec une précision sur l'ensemble d'apprentissages assez élevé 25%, 76 % et 92%. Donc, les modèles avec la primitive LBP/PCA obtiennent du résultat plus élevé avec 18.5% en validation et 19.2% en test pour un noyau SVM linéaire, 16.97% en validation et 17.4% avec un noyau SVM polynomial et 18.8% en validation et 17.4% avec un noyau SVM rbf. Pour ce qui est des modèles avec la primitive CNN, la meilleure précision est 16.2% en validation et 18.3% en test pour un noyau SVM rbf, mais quand même un surapprentissage à 91.9% de précision sur l'ensemble d'apprentissages, sinon il est suivi par le modèle SVM avec un noyau linière dont la précision est de 16.9% sur l'ensemble de validation et 17% sur l'ensemble de tests, puis ça termine avec le modèle avec un noyau polynomial pour une précision de 14.7% sur validation et 16.3% sur test."]},{"cell_type":"markdown","metadata":{"id":"UwnkLP3r1RhH"},"source":["### (1a): Code:"]},{"cell_type":"markdown","metadata":{"id":"8R1rEB571RhJ"},"source":["#### HandCrafted features"]},{"cell_type":"code","metadata":{"id":"Tpyemdqz1RhJ"},"source":["# Votre code ici\n","# FER primitives LBP\n","\"\"\"PCA\"\"\"\n","#LBP\n","PCA_lbp_vector_app = np.genfromtxt('PCA_lbp_vector_app.csv', delimiter=';')\n","PCA_lbp_vector_val = np.genfromtxt('PCA_lbp_vector_val.csv', delimiter=';')\n","PCA_lbp_vector_test = np.genfromtxt('PCA_lbp_vector_test.csv', delimiter=';')\n","\n","#Normalizer vector\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","\"\"\"PCA\"\"\"\n","PCA_lbp_vector_app = scaler.fit_transform(PCA_lbp_vector_app)\n","PCA_lbp_vector_val = scaler.fit_transform(PCA_lbp_vector_val)\n","PCA_lbp_vector_test = scaler.fit_transform(PCA_lbp_vector_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9NLYCiah1RhQ"},"source":["#### CNN features with normalization and reduced Dimension on CNN 5L 2FC"]},{"cell_type":"code","metadata":{"id":"PHuz6XaA1RhS","executionInfo":{"status":"error","timestamp":1587330061855,"user_tz":240,"elapsed":506,"user":{"displayName":"Imane Zriaa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioDY4dLkpnQFW2LMOcW3EpM23co77w9n8Vs1QD=s64","userId":"18312604134454482575"}},"outputId":"971a12ae-b455-42f3-a839-d3149ef5d3b8","colab":{"base_uri":"https://localhost:8080/","height":312}},"source":["PCA_Xflat_features_app = np.load(\"pca_deep_vector_app.npy\")\n","PCA_Xflat_features_val   = np.load(\"pca_deep_vector_val.npy\")\n","PCA_Xflat_features_test  = np.load(\"pca_deep_vector_test.npy\")"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-4b041fe2d2e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mPCA_Xflat_features_app\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pca_deep_vector_app.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mPCA_Xflat_features_val\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pca_deep_vector_val.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mPCA_Xflat_features_test\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pca_deep_vector_test.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pca_deep_vector_app.npy'"]}]},{"cell_type":"markdown","metadata":{"id":"QeZsI3tp1RhY"},"source":["#### Reduce Dimension -- Don't Run "]},{"cell_type":"code","metadata":{"id":"i9cKhKWd1Rha"},"source":["# from sklearn.decomposition import PCA\n","# from sklearn.preprocessing import StandardScaler\n","# import pandas as pd\n","\n","# Xflat_features_app = np.load(\"FER_Xflat_features_train.npy\")\n","# Xflat_features_val   = np.load(\"FER_Xflat_features_val.npy\")\n","# Xflat_features_test  = np.load(\"FER_Xflat_features_test.npy\")\n","\n","# def pcaTrans(vector_train):\n","#     pca = PCA(n_components=100)\n","#     principalComponents = pca.fit_transform(vector_train)\n","#     pca_vector = pca.transform( vector_train )\n","#     return principalComponents\n","\n","# PCA_Xflat_features_app = pcaTrans(Xflat_features_app)\n","# PCA_Xflat_features_test = pcaTrans(Xflat_features_test)\n","# PCA_Xflat_features_val = pcaTrans(Xflat_features_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d-jPHBPp1Rhf"},"source":["# np.save( \"pca_deep_vector_app.npy\", PCA_Xflat_features_app)\n","# np.save( \"pca_deep_vector_val.npy\", PCA_Xflat_features_test)\n","# np.save( \"pca_deep_vector_test.npy\", PCA_Xflat_features_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uWVXl2KJ1Rhl"},"source":["#Methode pour faire les différentes SVM necessaires\n","\n","def linearSVM_model():\n","    print(\"SVM with Linear Kernel (one-versus-one)\\n\")\n","    model = svm.SVC(C=10.0, cache_size=2000, class_weight=None, coef0=0.0,\n","                    decision_function_shape='ovo', degree=3, gamma='auto', kernel='linear',\n","                    max_iter=4000, probability=True, random_state=None, shrinking=False,\n","                    tol=0.001, verbose=True)\n","    return model\n","\n","def polySVM_model():\n","    print(\"SVM with Polynomial Kernel (one-versus-one)\\n\")\n","    model = svm.SVC(C=10.0, cache_size=2000, class_weight=None, coef0=0.0,\n","                    decision_function_shape='ovo', degree=3, gamma='auto', kernel='poly',\n","                    max_iter=4000, probability=True, random_state=None, shrinking=False,\n","                    tol=0.001, verbose=True)\n","    return model\n","\n","def rbfSVM_model():\n","    print(\"SVM with Gaussian Kernel(one-versus-one)\\n\")\n","    model = svm.SVC(C=10.0, cache_size=2000, class_weight=None, coef0=0.0,\n","                    decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\n","                    max_iter=4000, probability=True, random_state=None, shrinking=False,\n","                    tol=0.001, verbose=True)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t-ZP16Ez1Rho"},"source":["#### GPU version"]},{"cell_type":"code","metadata":{"id":"-qslmElw1Rhq"},"source":["def linearSVM_model():\n","    print(\"SVM with Linear Kernel (one-versus-one)\\n\")\n","    model = thundersvm.SVC(C=10.0, cache_size=2000, class_weight=None, coef0=0.0,\n","                    decision_function_shape='ovo', degree=3, gamma='auto', kernel='linear',\n","                    max_iter=4000, probability=True, random_state=None, shrinking=False,\n","                    tol=0.001, verbose=True)\n","    return model\n","\n","def polySVM_model():\n","    print(\"SVM with Polynomial Kernel (one-versus-one)\\n\")\n","    model = thundersvm.SVC(C=10.0, cache_size=2000, class_weight=None, coef0=0.0,\n","                    decision_function_shape='ovo', degree=3, gamma='auto', kernel='poly',\n","                    max_iter=4000, probability=True, random_state=None, shrinking=False, \n","                    tol=0.001, verbose=True)\n","    return model\n","\n","def rbfSVM_model():\n","    print(\"SVM with Gaussian Kernel(one-versus-one)\\n\")\n","    model = thundersvm.SVC(C=10.0, cache_size=2000, class_weight=None, coef0=0.0,\n","                    decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\n","                    max_iter=4000, probability=True, random_state=None, shrinking=False,\n","                    tol=0.001, verbose=True)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t5TBDICs1Rht","executionInfo":{"status":"error","timestamp":1587247557995,"user_tz":240,"elapsed":564,"user":{"displayName":"Imane Zriaa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioDY4dLkpnQFW2LMOcW3EpM23co77w9n8Vs1QD=s64","userId":"18312604134454482575"}},"outputId":"3c150019-d98d-4357-873d-0eb7aec3d69d","colab":{"base_uri":"https://localhost:8080/","height":128}},"source":["# Vos résultats ici:\n","#Permet de faire plusieurs tests de hyperparametres\n","param_grid_lin = [{'kernel': ['linear'], 'C': [10,50,100 256], 'max_iter': [4000]}]\n","param_grid_poly = [{'kernel': ['poly'], 'C': [10, 50,100,256], 'max_iter': [4000], 'degree':[3,6, 10]}]\n","param_grid_rbf = [{'kernel': ['rbf'], 'C': [10, 256], 'max_iter': [4000], 'gamma':[0.01, 0.05, 0.1]}]"],"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-35-538f08da1934>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    param_grid_lin = [{'kernel': ['linear'], 'C': [10,50,100 256], 'max_iter': [4000]}]\u001b[0m\n\u001b[0m                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"markdown","metadata":{"id":"74tIi7sn1Rhy"},"source":["### (1a): Résultats et résponses:"]},{"cell_type":"code","metadata":{"id":"HRUm73_01Rh0"},"source":["from sklearn.metrics import accuracy_score\n","def showResultat(tuned_model, vector) :\n","    print('The best performing parameters and their scores.')\n","    for z in tuned_model.get_param_scores()[:2]:\n","        p, s = z\n","        print(p)\n","        print('Score:', s)\n","    print('Verify that the lowest scoring parameters make sense.')\n","    for z in tuned_model.get_param_scores()[-2:]:\n","        p, s = z\n","        print(p)\n","        print('Score:', s)\n","    if (vector == \"handCrafted\"):\n","        # Use the tuned model to predict the class labels\n","        Ytrain_pred_tun = tuned_model.predict(PCA_lbp_vector_app)\n","        Yval_pred_tun   = tuned_model.predict(PCA_lbp_vector_val)\n","        Ytest_pred_tun  = tuned_model.predict(PCA_lbp_vector_test)\n","\n","    else:\n","        Ytrain_pred_tun = tuned_model.predict(Xflat_features_app)\n","        Yval_pred_tun   = tuned_model.predict(Xflat_features_val)\n","        Ytest_pred_tun  = tuned_model.predict(Xflat_features_test)\n","        \n","    # Final evaluation of the model (On the Training, Validation or Test dataset)\n","    scores_tun = accuracy_score(ytrain, Ytrain_pred_tun )\n","    print(\"Correct classification rate for the training dataset (tuned_model)   = \"+str(scores_tun*100)+\"%\")\n","\n","    scores2_tun = accuracy_score(yval, Yval_pred_tun )\n","    print(\"Correct classification rate for the validation dataset (tuned_model) = \"+str(scores2_tun*100)+\"%\")\n","\n","    scores3_tun = accuracy_score(ytest, Ytest_pred_tun )\n","    print(\"Correct classification rate for the test dataset (tuned_model) = \"+str(scores3_tun*100)+\"%\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1hfdZipLU_sv","executionInfo":{"status":"error","timestamp":1587247580592,"user_tz":240,"elapsed":541,"user":{"displayName":"Imane Zriaa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioDY4dLkpnQFW2LMOcW3EpM23co77w9n8Vs1QD=s64","userId":"18312604134454482575"}},"outputId":"4c3c3003-bbce-46b7-f97f-771f9d770a35","colab":{"base_uri":"https://localhost:8080/","height":228}},"source":[" # Final evaluation of the model (On the Training, Validation or Test dataset)\n","    scores_tun = accuracy_score(ytrain, Ytrain_pred_tun )\n","    print(\"Correct classification rate for the training dataset (tuned_model)   = \"+str(scores_tun*100)+\"%\")\n","\n","    scores2_tun = accuracy_score(yval, Yval_pred_tun )\n","    print(\"Correct classification rate for the validation dataset (tuned_model) = \"+str(scores2_tun*100)+\"%\")\n","\n","    scores3_tun = accuracy_score(ytest, Ytest_pred_tun )\n","    print(\"Correct classification rate for the test dataset (tuned_model) = \"+str(scores3_tun*100)+\"%\")"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-47d41af2f999>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores_tun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtrain_pred_tun\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Correct classification rate for the training dataset (tuned_model)   = \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_tun\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mscores2_tun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYval_pred_tun\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Correct classification rate for the validation dataset (tuned_model) = \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores2_tun\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Ytrain_pred_tun' is not defined"]}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"XjW2bsoS1Rh3","executionInfo":{"status":"error","timestamp":1587247543880,"user_tz":240,"elapsed":622,"user":{"displayName":"Imane Zriaa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioDY4dLkpnQFW2LMOcW3EpM23co77w9n8Vs1QD=s64","userId":"18312604134454482575"}},"outputId":"831ce848-4244-4a4d-bf53-bafbb5a3ac84","colab":{"base_uri":"https://localhost:8080/","height":228}},"source":["tuned_model_lin = GridSearch(linearSVM_model(), param_grid=param_grid_lin)\n","model_lin_deep = tuned_model_lin\n","\n","%time tuned_model_lin.fit(PCA_lbp_vector_app,ytrain)\n","%time model_lin_deep.fit(Xflat_features_app, ytrain)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-633b1f55a39c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtuned_model_lin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinearSVM_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid_lin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_lin_deep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuned_model_lin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time tuned_model_lin.fit(PCA_lbp_vector_app,ytrain)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# %time model_lin_deep.fit(Xflat_features_app, ytrain)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'linearSVM_model' is not defined"]}]},{"cell_type":"code","metadata":{"id":"-SKYpKV11Rh7"},"source":["tuned_model_poly = GridSearch(polySVM_model(), param_grid=param_grid_poly)\n","model_poly_deep = tuned_model_poly\n","\n","%time tuned_model_poly.fit(PCA_lbp_vector_app,ytrain)\n","%time model_poly_deep.fit(Xflat_features_app,ytrain)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9p2_qFPh1RiA"},"source":["tuned_model_rbf = GridSearch(rbfSVM_model(), param_grid=param_grid_rbf)\n","model_rbf_deep = rbfSVM_model\n","\n","%time tuned_model_rbf.fit(PCA_lbp_vector_app,ytrain)\n","%time model_rbf_deep.fit(Xflat_features_train, ytrain)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I_VGIq3e1RiD"},"source":["#### Results for handcrafted vectors"]},{"cell_type":"code","metadata":{"id":"M2f3PeYj1RiE"},"source":["showResultat(tuned_model_lin, \"handCrafted\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_pw7lySV1RiI"},"source":["showResultat(tuned_model_poly, \"handCrafted\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fwO8ytea1RiM"},"source":["showResultat(tuned_model_rbf, \"handCrafted\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JtflJ7r81RiO"},"source":["#### Results for Deep vectors"]},{"cell_type":"code","metadata":{"id":"rbbXSzOy1RiO"},"source":["showResultat(model_lin_deep, \"deep\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NW_Qj9Fw1RiT"},"source":["showResultat(model_poly_deep, \"deep\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y58Pnm1E1RiW"},"source":["showResultat(model_rbf_deep, \"deep\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2QQcg4z01RiZ"},"source":["## (1b) Créer et évaluer des modèles de régression\n","### (FG-NET dataset)"]},{"cell_type":"markdown","metadata":{"id":"O1Xj0fN71Rid"},"source":["Idéalement, vous devez tout refaire pour ce nouvel ensemble de données / tâche. Alors, pour simplifier ce TP, vous pouvez créer, optimiser et évaluer un seul modèle de régression. Vous pouvez choisir la meilleure combinaison représentation / SVM de la partie 1a. \n","<font color=blue>\n","##### À faire:\n","1. Choisir une combinaison primitive/kernel (Cellui qu'a meilleur performé dans la partie 1). \n","2. Entraîner et optimiser (C, epsilon, ???) un SRV (linéaire ou polynomial ou Gaussian)\n","3. Analyser les résultats et présenter vos conclusions sur le modèle SVR.\n","\n","|  Primitive   | Noyau    |   Paramètres       |  MAE   |  MSE   | \n","|--------------|----------|--------------------|--------|--------|\n","| FG_NET |  Lin     | C=100,ep=2               |8.286   |  144.864      |\n","</font>    \n","\n","###Analyse des résultats###\n","\n","Dans cette partie, la combinaison choisie pour effectuer le modèle SVR est en utilisant comme jeu de primitive le FG_NET 256 par 256 et comme kernel le linéaire. Le modèle SVR avec Kernel linéaire ajuste les coefficients de poids pour minimiser le résidu de somme des carrés entre cibles observées et prédites, avec comme paramètres un C=100 et epsilon de 2, le meansquarederror est de 144.864et la meanabsoluteerror est de 8.286 pour faire les prédictions. Les résultats pour ce modèle SVR montrent des erreurs moins élevées qu'au précédent avec l'algorithme de régression linéaire ordinaire."]},{"cell_type":"markdown","metadata":{"id":"Y-LXt4pq1Rid"},"source":["### (1b): Code:"]},{"cell_type":"markdown","metadata":{"id":"MwbFvsHe1Rif"},"source":["#### Preprocess data -- Taken from exemple "]},{"cell_type":"code","metadata":{"id":"Mg5Nh_kh1Rif"},"source":["X_data = np.loadtxt('fgnet_256x256.csv', delimiter=',', dtype=int)\n","Y_data = np.loadtxt('fgnet_labels.csv', delimiter=',', dtype=int)\n","Y_subj = np.loadtxt('fgnet_subjects.csv', delimiter=',', dtype=int)\n","\n","X_data = X_data.reshape(X_data.shape[0], 1, 256, 256).astype('uint8')\n","\n","# Adapted from Equipe 1 - Julien Robitaille and Bengoufa Mohamed Abdeldjalil\n","from skimage.feature import local_binary_pattern\n","import pathlib\n","\n","data_to_LBP = [\n","    (X_data, Y_data)\n","]\n","def format_nb(x: int):\n","    return str(int(x))\n","\n","X_data_lbp = np.zeros((1002 ,59))\n","\n","# Adapted from Equipe 1 - Julien Robitaille and Bengoufa Mohamed Abdeldjalil\n","#Local binary pattern feature to csv\n","#local_binary_pattern(image, P, R, method='default')\n","P= 8 \n","R= 2\n","\n","lbp_dim = local_binary_pattern(np.squeeze(X_data[0], axis=0), P, R, 'nri_uniform')\n","nb_bins = int(lbp_dim.max() + 1)\n","\n","number_of_points = 59\n","\n","X_data_lbp = np.zeros((1002 ,number_of_points))\n","\n","for data in data_to_LBP:\n","    x, y = data\n","    for i, image in enumerate(x):\n","        lbp = local_binary_pattern(image[0], P, R, 'nri_uniform') #Local binary pattern extraction\n","        hist, _ = np.histogram(lbp.ravel(), density=True, bins=nb_bins ,range=(0, nb_bins))\n","        hist = hist.astype(np.float)\n","        hist /= (hist.sum() + 1e-7)\n","        X_data_lbp[i,:] =  hist\n","        \n","from sklearn.preprocessing import MinMaxScaler\n","\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","X_data_lbp = scaler.fit_transform(X_data_lbp)\n","\n","X_data_lbp.shape, Y_data.shape, Y_subj.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5iKPVuIG1Ril"},"source":["def linearSVR_model():\n","    print(\"SVR with Linear Kernel\\n\")\n","    model = thundersvm.SVR(epsilon=0.1, C=10.0, cache_size=2000,\n","                    degree=2, kernel='linear',\n","                    max_iter=-1, shrinking=False,\n","                    tol=0.0001, verbose=True)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8GQdnHcu1Rio"},"source":["# Assuming you already have training, validation and test sets, as well as a \"model\".\n","from hypopt import GridSearch\n","param_grid = [{'kernel': ['linear'], 'C': [1, 10, 100, 1000], 'epsilon': [0.01, 0.1, 0.2, 0.25, 0.5, 1, 2, 5]}]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"LGuWUgLg1Ris"},"source":["model       = linearSVR_model()\n","tuned_model = GridSearch(model = model, param_grid = param_grid)\n","\n","%time tuned_model.fit(X_data_lbp, Y_data, scoring=\"neg_mean_absolute_error\", scoring_params = None, verbose=True )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fjBbcT3f1Riv"},"source":["# Use the model to predict just the class labels\n","Y_data_pred = tuned_model.predict(X_data_lbp)\n","\n","# Compute the MAE and MSE metrics for Regression\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","\n","def MSE(y_true,y_pred):\n","    mse = mean_squared_error(y_true, y_pred)\n","    # print('MSE: %2.5f' % mse)\n","    return mse\n","\n","def MAE(y_true,y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    # print('MAE: %2.5f' % mse)\n","    return mae\n","\n","# Final evaluation of the model (On the Training, Validation or Test dataset)\n","scores = MAE( Y_data, Y_data_pred )\n","print(\"MAE for the training dataset   = \"+str(scores)+\" years\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q7xqp0C_1Ri0"},"source":["print('The best performing parameters and their scores.')\n","for z in tuned_model.get_param_scores()[:1]:\n","    p, s = z\n","    print(p)\n","    print('Score:', s)\n","print()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ElZso3t1Ri7"},"source":["### (1b): Résultats et résponses:"]},{"cell_type":"code","metadata":{"id":"lgTxQ5Gc1Ri8"},"source":["# Use the tuned model to predict the class labels\n","Y_data_pred_tun = tuned_model.predict(X_data_lbp)\n","# Final evaluation of the model (On the Training, Validation or Test dataset)\n","scores_tun = MAE(Y_data, Y_data_pred_tun  )\n","mse =  MSE(Y_data, Y_data_pred_tun)\n","\n","print(\"MAE for the dataset (model)       = \"+str(scores)+\" years\")\n","print(\"MAE for the dataset (tuned_model) = \"+str(scores_tun)+\" years\")\n","print(\"MSE for the dataset (tuned_model) = \"+str(mse)+\" years\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kjrp-wm1Gkmd"},"source":["### (1)Analyse"]},{"cell_type":"markdown","metadata":{"id":"sVdF5sl2Gni4"},"source":["Voici les résultats obtenus pour l'algorithme de régression avec (FG-NET dataset) ainsi que pour les algorithmes de classification (FER dataset) dans la partie 1 du laboratoire.\n","\n","|  Primitive   | Noyau    |   Paramètres       |  MAE   |  MSE   | Comparaison | Primitive   | Noyau       |   Paramètres    |  % App | % Val  | % Tst  | \n","|--------------|----------|--------------------||--------|--------|--------------|-------------|-----------------|--------|--------|--------|\n","| FG_NET |  Lin     | C=100,ep=2               |8.286   |  144.864       |----| LBP avec PCA | lineaire    | C=256            | 19.123 | 18.501 | 19.170       |\n","||||||----| LBP avec PCA | polynomial  | C=10,d=6         |  17.311 |16.968        | 17.414       |\n","||||||----| LBP avec PCA | RBF         | C=256, g=0.01   | 18.865 | 18.807 | 17.414 |\n","||||||----| CNN 5L 2FC     | lineaire    | C=10            | 25.37 | 16.912 |  17.024      |\n","||||||----| CNN 5L 2FC     | polynomial  | C=10,d=3         |  76.683 | 14.712       |  16.299      |\n","||||||----| CNN 5L 2FC     | RBF         | C=10,d=3,g=0.05   | 91.946 |16.188 | 18.278     |\n","\n","Pour ce qui est de la partie 1 avec la régression sur les données FG-NET, les principaux résultats qu'on peut observer sont avec la régression SVR avec un noyau linéaire dont la MSE obtenue est de 144.864 et MAE obtenue est de 8.286. Puis, pour la partie avec la classification sur les données FER, avec les primitives LBP avec transformation PCA, les principaux résultats avec le modèle SVM sont avec une précision entre 17% et 19% avec les noyaux linéaire, polynomial et rbf et ceux avec les vecteurs de primitives CNN sont proche avec une précision entre 15% et 18%, toutefois l'erreur reste assez élevée pour les modèles. "]},{"cell_type":"markdown","metadata":{"id":"gaHbIamq1Ri_"},"source":["# Partie 2: Combinaison des algorithmes d'apprentissage\n","## (FER dataset)\n","\n","Vous devrez intégrer une notion supplémentaire acquise récemment dans le cours théorique, à savoir la combinaison de modèles d’apprentissage. Contrairement aux laboratoires précédents dans lesquelles vous travailliez avec des modèles indépendants sur un seul ensemble de données, vous devez dans ce présent laboratoire combiner des algorithmes d’apprentissage afin de classifier les échantillons d’un ensemble de données regroupant plusieurs sortes de primitives (feature sets). Vous implémenterez ce système avec les librairies TensorFlow ou scikit-learn. Finalement, vous aurez à proposer une stratégie combinatoire de classificateurs tel que vote de majorité, moyenne, produit, médiane, etc."]},{"cell_type":"markdown","metadata":{"id":"X6LfrIQj1Ri_"},"source":["## (2a) Créer et évaluer des ensembles Bagging et Boosting"]},{"cell_type":"markdown","metadata":{"id":"soitu5_51RjA"},"source":["<font color=blue>\n","    \n","##### À faire:\n","1. Choisir un algorithme de base (AD, NB, KNN) pour construire des ensembles avec \"bagging\" et \"boosting\"\n","2. Entraîner et optimiser les ensembles bagging et Adaboost\n","3. Comparer la performance des ensembles avec des classificateurs individuels.     \n","4. Analyser les résultats et présenter vos conclusions sur les ensembles bagging et boosting.\n","\n","|  Primitive   | Classificateur | Ensemble |  paramètres |  % App | % Val  | % Tst  | \n","|--------------|----------------|----------|-------------|--------|--------|--------|\n","| LBP avec PCA |       AD       | bagging  |ms=0.4,ne=50 | XXX.XX | XXX.XX | XXX.XX |\n","| LBP avec PCA |       AD       | adaboost |lr=1.0,ne=200| XXX.XX | XXX.XX | XXX.XX |    \n","| ...          |                |                        | XXX.XX | XXX.XX | XXX.XX |    \n","</font>    \n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZQAqcJwcSKrj"},"source":["## **Analyse des résultats**\n"]},{"cell_type":"markdown","metadata":{"id":"Hy2QbOdcSQwx"},"source":["Le Bagging (Bootstrap Aggregation) et l'Adaboost sont des techniques d'optimisation utilisées pour améliorer la classification notamment celle avec des arbres de décision, qui representaient des classificateurs très faibles. \n","Le** Bagging** a pour but de ** réduire la variance** de l’estimateur alors que **l'Adaboost** c'est pour **minimiser le biais** de l'estimateur.\n","Neanmoins, dans notre cas la performance de ces deux techniques étaient à peu près la meme chose vu la faible dimension de nos vecteurs primitives(LBP avec PCA) ; apres avoir optimiser les hyperparametres sur l'ensemble de validatiion,on obtient pour un classificateur AD avec un bagging  25.55% pour les données d'apprentissage contre 25.59% pour AD avec un Adaboost \n","et 24,85% pour les données test pour le bagging contre 23.26% pour l'Adaboost.\n","\n","\n","\n","L'utilisation de ces deux ensembles dans notre cas n'a pas trop augmenter la performance de notre AD individuel(25.61% pour les données d'apprentissage  et 24.77% pour les données test) et cela est expliqué par **la faible dimension de nos vecteurs primitives ce qui n'assure pas suffisament de diversité.**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GZtL3OwMSBxW"},"source":["<font color=black>\n","\n","|  Primitive   | Classificateur | Ensemble |  paramètres |  % App | % Val  | % Tst  | \n","|--------------|----------------|----------|-------------|--------|--------|--------|\n","| LBP avec PCA |       AD       | bagging  |ms=0.4,mf=0.5,ne=50,nj=8 |25.55%|25.35%|24.85%|\n","| LBP avec PCA |       AD       | adaboost |lr=1.0,ne=10|25.59%  | 22.73% | 23.26%|    \n","|   LBP avec PCA       |     AD individuel           |                        | |25.61%|25.10% | 24.77% |\n","| LBP avec PCA |       NB individuel    |  || 25.27% | 22.09% | 22.48% | \n","| LBP avec PCA |       KNN individuel      |  || 44,99% | 18.97% | 20.78% |\n","| LBP avec PCA |       Random Forest    |  |  |32.06% | 18.61% | 19.0025%\n","\n","\n","<front>"]},{"cell_type":"markdown","metadata":{"id":"VOmQryJr4fon"},"source":["Généralement les ensembles (Bagging et Adaboost) avec un AD et plus performant sur les données test qu'aux autres classificateurs individuels tel est le cas de KNN(20.78%), NB(22.48%),RF(19.0025%)"]},{"cell_type":"markdown","metadata":{"id":"saAiymGY1RjB"},"source":["### <font color=blue> (2a) Code: </font> "]},{"cell_type":"code","metadata":{"id":"GFlsRJvl0lrQ","executionInfo":{"status":"ok","timestamp":1587342763814,"user_tz":240,"elapsed":3768,"user":{"displayName":"Imane Zriaa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioDY4dLkpnQFW2LMOcW3EpM23co77w9n8Vs1QD=s64","userId":"18312604134454482575"}},"outputId":"b7ff577b-22d8-4a33-8a33-c72ca83293a4","colab":{"base_uri":"https://localhost:8080/","height":120}},"source":["pip install hypopt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: hypopt in /usr/local/lib/python3.6/dist-packages (1.0.9)\n","Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from hypopt) (0.22.2.post1)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from hypopt) (1.18.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->hypopt) (0.14.1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->hypopt) (1.4.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hPSHicN10m3R"},"source":["\"On choisit les hyperparametres par une 'cross-validation' pour optimiser notre algorithme d'Arbre de décision\"\n","# Set the parameters by cross-validation\n","# Assuming you already have train, test, val sets and a model.\n","from hypopt import GridSearch\n","\n","tuned_parameters = [{'max_depth': [2, 4, 6], \n","                     'min_samples_leaf': [10,20 ], \n","                     'min_samples_split': [5, 10, 20]}]\n","\n","#DecisionTreeClassifier(criterion='gini', max_depth=4 ,min_samples_leaf=10, random_state=1)                     "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hWh1oJIP0yFp","executionInfo":{"status":"ok","timestamp":1587345474765,"user_tz":240,"elapsed":2273,"user":{"displayName":"Imane Zriaa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioDY4dLkpnQFW2LMOcW3EpM23co77w9n8Vs1QD=s64","userId":"18312604134454482575"}},"outputId":"b6a12a1b-4579-4eb1-b37d-80e22cd61e0a","colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["\"Un Grid search pour trouver les parametres qui optimise notre modele \"\n","# Grid-search all parameter combinations using a validation set.\n","\n","tuned_model = GridSearch(model = tree.DecisionTreeClassifier(criterion='entropy'),param_grid = tuned_parameters)\n","tuned_model.fit(PCA_lbp_vector_app, ytrain, PCA_lbp_vector_val, yval)\n","\n","print(\"Best parameters set found on validation set:\")\n","print()\n","print('Test Score for Optimized Parameters:', tuned_model.score(PCA_lbp_vector_val, yval)) #"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 18/18 [00:01<00:00, 12.17it/s]"],"name":"stderr"},{"output_type":"stream","text":["Best parameters set found on validation set:\n","\n","Test Score for Optimized Parameters: 0.251044859292282\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"J8jmEeY305Eo","executionInfo":{"status":"ok","timestamp":1587345477381,"user_tz":240,"elapsed":565,"user":{"displayName":"Imane Zriaa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioDY4dLkpnQFW2LMOcW3EpM23co77w9n8Vs1QD=s64","userId":"18312604134454482575"}},"outputId":"75c9ac82-149e-425c-be9e-0c87f8772d71","colab":{"base_uri":"https://localhost:8080/","height":201}},"source":["print('We can view the best performing parameters and their scores.')\n","for z in tuned_model.get_param_scores()[:2]:\n","    p, s = z\n","    print(p)\n","    print('Score:', s)\n","print()\n","print('Verify that the lowest scoring parameters make sense.')\n","for z in tuned_model.get_param_scores()[-2:]:\n","    p, s = z\n","    print(p)\n","    print('Score:', s)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["We can view the best performing parameters and their scores.\n","{'max_depth': 4, 'min_samples_leaf': 10, 'min_samples_split': 20}\n","Score: 0.251044859292282\n","{'max_depth': 4, 'min_samples_leaf': 10, 'min_samples_split': 5}\n","Score: 0.251044859292282\n","\n","Verify that the lowest scoring parameters make sense.\n","{'max_depth': 6, 'min_samples_leaf': 20, 'min_samples_split': 5}\n","Score: 0.20897185845639454\n","{'max_depth': 6, 'min_samples_leaf': 20, 'min_samples_split': 20}\n","Score: 0.20897185845639454\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-dGrI8EN1JkV","executionInfo":{"status":"ok","timestamp":1587352636133,"user_tz":240,"elapsed":787,"user":{"displayName":"Imane Zriaa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioDY4dLkpnQFW2LMOcW3EpM23co77w9n8Vs1QD=s64","userId":"18312604134454482575"}},"outputId":"d698488d-4d0b-4213-dc4a-1fc277a5dd1c","colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["scores_tuned  = accuracy_score(ytrain, Ytrain_pred )\n","scores2_tuned = accuracy_score(yval, Yvalid_pred)\n","scores3_tuned = accuracy_score(ytest, Ytest_pred )\n","print(scores_tuned ,scores2_tuned ,scores3_tuned )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.2561566059423874 0.251044859292282 0.24770130955697967\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gDwO823s1Euj"},"source":["# Use the tunes model to predict the class of samples\n","\n","Ytrain_pred = tuned_model.predict(PCA_lbp_vector_app)\n","Yvalid_pred = tuned_model.predict(PCA_lbp_vector_val)\n","Ytest_pred  = tuned_model.predict(PCA_lbp_vector_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wmTnx1Vj60HW"},"source":["model = DT_model() # Build the model\n","model.fit(PCA_lbp_vector_app,ytrain) # Fit the model (TRAIN)\n","Ytrain_pred = model.predict(PCA_lbp_vector_app)#predict the class of samples\n","Yvalid_pred = model.predict(PCA_lbp_vector_val)\n","Ytest_pred  = model.predict(PCA_lbp_vector_test)                    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sxCT3fhgFDyb"},"source":["**BAGGING**"]},{"cell_type":"code","metadata":{"id":"DBQB3nq_LoBF"},"source":["\"On utilise un AD pour construire un ensemble BAGGING\"\n","#BAGGING\n","from sklearn.ensemble import BaggingClassifier\n","# Using a bagging of Decision Trees\n","\n","#bagging = BaggingClassifier(tree.DecisionTreeClassifier(criterion='entropy', max_depth=15, min_samples_leaf=5, min_samples_split=10),\n","bagging = BaggingClassifier(tree.DecisionTreeClassifier(criterion='entropy', max_depth=3, min_samples_leaf=10, random_state=1),\n","                            max_samples  = 0.4,\n","                             max_features = 0.5,\n","                            n_estimators = 50,\n","                            n_jobs = 8,\n","                            bootstrap = False)\n","#model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=4, min_samples_leaf=10, random_state=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0HgNTUJzN_qc","executionInfo":{"status":"ok","timestamp":1587354300029,"user_tz":240,"elapsed":1675,"user":{"displayName":"Imane Zriaa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioDY4dLkpnQFW2LMOcW3EpM23co77w9n8Vs1QD=s64","userId":"18312604134454482575"}},"outputId":"ddb4748e-f5ce-400d-ca6b-47d8b1358dd7","colab":{"base_uri":"https://localhost:8080/","height":301}},"source":["# Fit the model (TRAIN)\n","bagging.fit(PCA_lbp_vector_app,ytrain)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BaggingClassifier(base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n","                                                        class_weight=None,\n","                                                        criterion='entropy',\n","                                                        max_depth=3,\n","                                                        max_features=None,\n","                                                        max_leaf_nodes=None,\n","                                                        min_impurity_decrease=0.0,\n","                                                        min_impurity_split=None,\n","                                                        min_samples_leaf=10,\n","                                                        min_samples_split=2,\n","                                                        min_weight_fraction_leaf=0.0,\n","                                                        presort='deprecated',\n","                                                        random_state=1,\n","                                                        splitter='best'),\n","                  bootstrap=False, bootstrap_features=False, max_features=0.5,\n","                  max_samples=0.4, n_estimators=50, n_jobs=8, oob_score=False,\n","                  random_state=None, verbose=0, warm_start=False)"]},"metadata":{"tags":[]},"execution_count":346}]},{"cell_type":"code","metadata":{"id":"x2HVXBSKOB7H"},"source":["\n","# Use the model to predict the class of samples\n","\n","Ytrain_pred_bagging = bagging.predict(PCA_lbp_vector_app)\n","Yvalid_pred_bagging = bagging.predict(PCA_lbp_vector_val)\n","Ytest_pred_bagging  = bagging.predict(PCA_lbp_vector_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pwloQDHBGSTt"},"source":["**AdaBoost**"]},{"cell_type":"code","metadata":{"id":"1waJs_wUONub"},"source":["\"On construit avec le classificateur AD un ensemble AdaBoost\"\n","#AdaBoost\n","from sklearn.ensemble import AdaBoostClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"liAl1aEnOexT"},"source":["# Using a AdaBoosting of Decision Trees\n","\n","#AdaB= AdaBoostClassifier(tree.DecisionTreeClassifier(criterion='entropy', max_depth=15, min_samples_leaf=5, min_samples_split=10),\n","AdaB= AdaBoostClassifier(tree.DecisionTreeClassifier(criterion='entropy', max_depth=3\n","                                                     , min_samples_leaf=10, random_state=1), \n","                         n_estimators = 10, \n","                         learning_rate = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2laLZu2QOhVA","executionInfo":{"status":"ok","timestamp":1587354833506,"user_tz":240,"elapsed":1493,"user":{"displayName":"Imane Zriaa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioDY4dLkpnQFW2LMOcW3EpM23co77w9n8Vs1QD=s64","userId":"18312604134454482575"}},"outputId":"c9e52c1a-3037-43f4-86b3-8181e19060dc","colab":{"base_uri":"https://localhost:8080/","height":284}},"source":["# Fit the model to the AdaB (TRAIN)\n","\n","AdaB.fit(PCA_lbp_vector_app,ytrain)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AdaBoostClassifier(algorithm='SAMME.R',\n","                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n","                                                         class_weight=None,\n","                                                         criterion='entropy',\n","                                                         max_depth=3,\n","                                                         max_features=None,\n","                                                         max_leaf_nodes=None,\n","                                                         min_impurity_decrease=0.0,\n","                                                         min_impurity_split=None,\n","                                                         min_samples_leaf=10,\n","                                                         min_samples_split=2,\n","                                                         min_weight_fraction_leaf=0.0,\n","                                                         presort='deprecated',\n","                                                         random_state=1,\n","                                                         splitter='best'),\n","                   learning_rate=1, n_estimators=10, random_state=None)"]},"metadata":{"tags":[]},"execution_count":391}]},{"cell_type":"code","metadata":{"id":"fMVwtwucOjgx"},"source":["# Use the model to predict the class of samples\n","# Notice that we are testing the train dataset\n","\n","Ytrain_pred_adaboost = AdaB.predict(PCA_lbp_vector_app)\n","\n","Yvalid_pred_adaboost = AdaB.predict(PCA_lbp_vector_val)\n","Ytest_pred_adaboost  = AdaB.predict(PCA_lbp_vector_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2iU5tEklL6cx"},"source":["**`Autres classificateurs individuels`**"]},{"cell_type":"code","metadata":{"id":"aPpTBO5aK-Xf"},"source":["# Training classifiers\n","clf2 = RandomForestClassifier(n_estimators=50, max_depth=12, min_samples_split=2, random_state=1)\n","clf3 = GaussianNB()\n","clf4 = KNeighborsClassifier(n_neighbors=20,weights = 'uniform',metric = 'euclidean', algorithm='kd_tree')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TvPTpa7u1RjF"},"source":["### <font color=blue> (2a): Résultats et résponses: </font> \n"]},{"cell_type":"code","metadata":{"id":"jkjMwH68HsTm","executionInfo":{"status":"ok","timestamp":1587354841365,"user_tz":240,"elapsed":569,"user":{"displayName":"Imane Zriaa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioDY4dLkpnQFW2LMOcW3EpM23co77w9n8Vs1QD=s64","userId":"18312604134454482575"}},"outputId":"3d99c9b1-f655-4899-cfef-83fb902e5e64","colab":{"base_uri":"https://localhost:8080/","height":234}},"source":["#On compare la performance des classificateurs AD , AD+Bagging et AD+AdaBoost\n","\n","'''classificateur AD individuel '''\n","scores = accuracy_score(ytrain, Ytrain_pred )\n","scores2 = accuracy_score(yval, Yvalid_pred )\n","scores3 = accuracy_score(ytest, Ytest_pred )\n","\n","\n","'''classificateur AD + BAGGING '''\n","scores_bagging  = accuracy_score(ytrain, Ytrain_pred_bagging)\n","scores2_bagging = accuracy_score(yval, Yvalid_pred_bagging)\n","scores3_bagging = accuracy_score(ytest, Ytest_pred_bagging)\n","\n","\n","'''classificateur AD+AdaBoost'''\n","scores_adaboost  = accuracy_score(ytrain, Ytrain_pred_adaboost )\n","scores2_adaboost = accuracy_score(yval, Yvalid_pred_adaboost )\n","scores3_adaboost = accuracy_score(ytest, Ytest_pred_adaboost )\n","\n","\n","print(\"Correct classification rate for the training dataset (AD individuel)      = \"+str(scores*100)+\"%\")\n","print(\"Correct classification rate for the training dataset (AD+bagging)     = \"+str(scores_bagging*100)+\"%\")\n","print(\"Correct classification rate for the training dataset (AD+adaboost)    = \"+str(scores_adaboost*100)+\"%\")\n","print()\n","print()\n","print(\"Correct classification rate for the validation dataset (AD individuel)      = \"+str(scores2*100)+\"%\")\n","print(\"Correct classification rate for the validation dataset (AD+bagging)     = \"+str(scores2_bagging*100)+\"%\")\n","print(\"Correct classification rate for the validation dataset (AD+adaboost)    = \"+str(scores2_adaboost*100)+\"%\")\n","print()\n","print()\n","print(\"Correct classification rate for the test dataset (AD individuel)      = \"+str(scores3*100)+\"%\")\n","print(\"Correct classification rate for the test dataset (AD +bagging)     = \"+str(scores3_bagging*100)+\"%\")\n","print(\"Correct classification rate for the test dataset (AD +adaboost)    = \"+str(scores3_adaboost*100)+\"%\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Correct classification rate for the training dataset (AD individuel)      = 25.615660594238744%\n","Correct classification rate for the training dataset (AD+bagging)     = 25.552962485631685%\n","Correct classification rate for the training dataset (AD+adaboost)    = 25.598244452959%\n","\n","\n","Correct classification rate for the validation dataset (AD individuel)      = 25.104485929228197%\n","Correct classification rate for the validation dataset (AD+bagging)     = 24.993034271384786%\n","Correct classification rate for the validation dataset (AD+adaboost)    = 22.736138200055724%\n","\n","\n","Correct classification rate for the test dataset (AD individuel)      = 24.770130955697965%\n","Correct classification rate for the test dataset (AD +bagging)     = 25.43884090275843%\n","Correct classification rate for the test dataset (AD +adaboost)    = 23.265533574811926%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rD7GktyTLIpz","executionInfo":{"status":"ok","timestamp":1587353150198,"user_tz":240,"elapsed":3435,"user":{"displayName":"Imane Zriaa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioDY4dLkpnQFW2LMOcW3EpM23co77w9n8Vs1QD=s64","userId":"18312604134454482575"}},"outputId":"3acc9780-79ab-4c2b-e60b-e5039314edf4","colab":{"base_uri":"https://localhost:8080/","height":217}},"source":["# Fit the Ensemble (TRAIN)\n","# Sert seulement à l'évaluation les performances d classificateurs simples\n","clf2.fit(PCA_lbp_vector_app, ytrain)\n","scores_tr_ENSEMBLE  = accuracy_score(ytrain, clf2.predict(PCA_lbp_vector_app) )\n","scores_val_ENSEMBLE = accuracy_score(yval, clf2.predict(PCA_lbp_vector_val) )\n","scores_tst_ENSEMBLE = accuracy_score(ytest,  clf2.predict(PCA_lbp_vector_test) )\n","print()\n","print(\"Classification rate for the training dataset (RT)      = \"+str(scores_tr_ENSEMBLE*100)+\"%\")\n","print(\"Classification rate for the validation dataset  (RT)     = \"+str(scores_val_ENSEMBLE*100)+\"%\")\n","print(\"Classification rate for the test dataset (RT)      = \"+str(scores_tst_ENSEMBLE*100)+\"%\")\n","clf3.fit(PCA_lbp_vector_app, ytrain)\n","scores_tr_ENSEMBLE  = accuracy_score(ytrain, clf3.predict(PCA_lbp_vector_app) )\n","scores_val_ENSEMBLE = accuracy_score(yval, clf3.predict(PCA_lbp_vector_val) )\n","scores_tst_ENSEMBLE = accuracy_score(ytest,  clf3.predict(PCA_lbp_vector_test) )\n","print()\n","print(\"Classification rate for the training dataset (NB)      = \"+str(scores_tr_ENSEMBLE*100)+\"%\")\n","print(\"Classification rate for the validation dataset  (NB)     = \"+str(scores_val_ENSEMBLE*100)+\"%\")\n","print(\"Classification rate for the test dataset (NB)      = \"+str(scores_tst_ENSEMBLE*100)+\"%\")\n","clf4.fit(PCA_lbp_vector_app, ytrain)\n","scores_tr_ENSEMBLE  = accuracy_score(ytrain, clf4.predict(PCA_lbp_vector_app) )\n","scores_val_ENSEMBLE = accuracy_score(yval, clf4.predict(PCA_lbp_vector_val) )\n","scores_tst_ENSEMBLE = accuracy_score(ytest,  clf4.predict(PCA_lbp_vector_test) )\n","print()\n","print(\"Classification rate for the training dataset (KNN)      = \"+str(scores_tr_ENSEMBLE*100)+\"%\")\n","print(\"Classification rate for the validation dataset  (KNN)     = \"+str(scores_val_ENSEMBLE*100)+\"%\")\n","print(\"Classification rate for the test dataset (KNN)      = \"+str(scores_tst_ENSEMBLE*100)+\"%\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Classification rate for the training dataset (RT)      = 44.99285938207531%\n","Classification rate for the validation dataset  (RT)     = 18.974644747840625%\n","Classification rate for the test dataset (RT)      = 20.78573418779604%\n","\n","Classification rate for the training dataset (NB)      = 25.277787453411822%\n","Classification rate for the validation dataset  (NB)     = 22.095291167456114%\n","Classification rate for the test dataset (NB)      = 22.48537196990805%\n","\n","Classification rate for the training dataset (KNN)      = 32.06659932425372%\n","Classification rate for the validation dataset  (KNN)     = 18.612426859849542%\n","Classification rate for the test dataset (KNN)      = 19.002507662301475%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"S99ZxLjw1RjL"},"source":["## <font color=blue> (2b) Créer et évaluer des combinaison de modèles </font>"]},{"cell_type":"markdown","metadata":{"id":"oDYfn00e1RjM"},"source":["<font color=blue>\n","    \n","##### À faire:\n","\n","1. Réalisez une combinaison de au moins 3 modèles d’apprentissage (p.ex. AD, NB et kNN). Pour ce faire, vous pouvez\n","combiner des modèles précédemment élaborés ou en générer d’autres. Au moins 3\n","modèles doivent faire partie de votre combinaison.\n","2. Choisir au moins deux règles de combinaison. Votre règle de décision peut être inspirée des notes de cours ou de la librairie scikit-learn et de sa documentation, pourvu que vous justifiiez le choix de celle-ci.\n","3. Produisez un diagramme afin d’illustrer votre stratégie d’ensemble qui permet clairement de voir quels ensembles de primitives vous avez choisis d’inclurent, quels classificateurs font partie de votre solution et quelle stratégie de combinaison vous avez choisie. Joignez-le à ce notebook.\n","\n","<img src=\"picture.png\" style=\"height:600px\">\n","    \n","4. Exportez vos modèles. Cet exemple vous aidera à exporter vos modèles à l’aide de la librairie scikit-learn, alors que ce lien vous aidera avec TensorFlow.\n","5. Comparer la performance des ensembles avec des classificateurs individuels et des ensembles (partie 1).     \n","6. Analyser les résultats et présenter vos conclusions sur les combinaisons de modèles.\n","</font>\n","\n","\n","Utilisation du VotingClassifier de la librairie scikit-learn pour faire la combinaison de modèles selon deux différentes règles soit __le vote par majorité__ et __la votation pondérée sur les classes prédites__, le jeu de primitives PCA_lbp est le vecteur de données pour faire l'évaluation de ce classificateur par votation avec quatre différents modèles de classification, soit le DecisionTreeClassifier, le randomForestClassifier, le gaussian naives bayes et KNeighborsClassifier.\n","\n","|JEU DE PRIMITIVES| CLASSIFICATEUR |COMBINAISON |RÉSULTATS|\n","|-|-|-|-|                   \n","||                   ---->DT------>\n","||                   ---->RT------>\n","|PCA_lbp -------->|================|--->VotingClassifier--->|y\n","||                   ---->NB------>\n","||                   ---->nKK----->\n","\n","|JEU DE PRIMITIVES| CLASSIFICATEUR |COMBINAISON |RÉSULTATS|\n","|-|-|-|-|                   \n","||                   ---->DT------>(1.8)\n","||                   ---->RT------>(0.8)\n","|PCA_lbp -------->|================|--->VotingClassifier--->|y\n","||                   ---->NB------>(1.2)\n","||                   ---->nKK----->(0.8)\n","\n","La performance par les six classificateurs individuels de la partie 1 sont moins élevé avec des précisions obtenues entre 16% et 18% pour l'ensemble validation et l'ensemble test, tandis que les ensembles obtiennent au moins une précision de 22% et 24% pour les ensembles de validation et de test, ce qui représente une différence d'au moins 4% entre les classificateurs simples et des ensembles.\n","\n","|  Primitive   | Noyau       |   Paramètres    |  % App  |  % Val | % Tst | _VS_ |  Primitive   | Classificateurs |   Règle  |  paramètres |  % App | % Val  | % Tst  |\n","|--------------|-------------|-----------------|---------|--------|--------|---|--------------|------------------|---------|-------------|--------|---------|--------|\n","| LBP avec PCA | lineaire    | C=256            | 19.123 | 18.501 | 19.170 |----| LBP avec PCA |  DT, RT, NB, kNN | votation majorité |'hard' | 31.282 | 22.597 | 22.820 |\n","| LBP avec PCA | polynomial  | C=10,d=6         |  17.311 |16.968 | 17.414 |----| LBP avec PCA |  DT, RT, NB, kNN | votation pondérée |'hard''1.8,0.8,1.2,0.8'| 27.214 | 24.463 | 24.408 |\n","| LBP avec PCA | RBF         | C=256, g=0.01    | 18.865 | 18.807 | 17.414 |----|--------------|------------------|--------------------| |--------|---------|--------|\n","| CNN 5L 2FC   | lineaire    | C=10             | 25.37 | 16.912 | 17.024 |----|--------------|------------------|--------------------| |--------|---------|--------|\n","| CNN 5L 2FC   | polynomial  | C=10,d=3         |  76.683 | 14.712 | 16.299|----|--------------|------------------|--------------------| |--------|---------|--------|\n","| CNN 5L 2FC   | RBF         | C=10,d=3,g=0.05  | 91.946_|16.188_| 18.278_|----|--------------|------------------|--------------------||--------|---------|--------|\n","\n","Avec le premier modèle de combinaisons fait avec comme paramètres la votation par majorité(soit voting='hard') et le jeu de primitives LBP avec PCA, les résultats que nous pouvons avoir sont un pourcentage d'efficacité de 31.28% pour l'ensemble d'entrainement, un pourcentage d'efficacité de 22.60% pour l'ensemble de validation et un pourcentage d'efficacité de 22.82% pour l'ensemble de test. Ce qui est pour le deuxième modèle avec comme paramètres la votation par majorité avec des poids pondérés(soit voting='hard',weights=[1.8,0.8,1.2,0.8]), nous avons obtenu un pourcentage de 27.21% pour l'ensemble d'entrainement, un pourcentage de 24.46% pour l'ensemble de validation et un pourcentage de 24.41% pour l'ensemble de test. Donc, avec les résultats de deux classificateurs d'ensembles, on remarque que la précision obtenue pour les ensembles de validation et de test est 2% plus élevé pour celui dont la règle de votation pondérée est appliquée."]},{"cell_type":"markdown","metadata":{"id":"KZar7Paz1RjM"},"source":["###  (2b): Code:"]},{"cell_type":"code","metadata":{"id":"EbbZwd471RjN","executionInfo":{"status":"ok","timestamp":1587335513738,"user_tz":240,"elapsed":8928,"user":{"displayName":"Imane Zriaa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioDY4dLkpnQFW2LMOcW3EpM23co77w9n8Vs1QD=s64","userId":"18312604134454482575"}},"outputId":"26cd4699-27c4-42ed-8550-3479086036d5","colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["# Training classifiers\n","clf1 = DecisionTreeClassifier(criterion='gini', max_depth=4 ,min_samples_leaf=10, random_state=1)\n","clf2 = RandomForestClassifier(n_estimators=50, max_depth=12, min_samples_split=2, random_state=1)\n","clf3 = GaussianNB()\n","clf4 = KNeighborsClassifier(n_neighbors=20,weights = 'uniform',metric = 'euclidean', algorithm='kd_tree')\n","# Training ensemble\n","model_votation_majority = VotingClassifier(estimators=[('dt', clf1), ('rf',clf2),('nb', clf3), ('knn', clf4)], voting='hard', n_jobs=-1)\n","\n","model_predict_proba_votation = VotingClassifier(estimators=[('dt', clf1), ('rf',clf2),('nb', clf3), ('knn', clf4)], voting='hard',weights=[1.8,0.8,1.2,0.8], n_jobs=-1)\n","\n","#params = {'dt__max_depth': [10,20,40,50], 'dt__min_samples_leaf': [2,10,20], 'dt__min_samples_split': [2,10,20], 'rf__n_estimators': [10, 200], 'rf__max_depth': [10,20,40,50] ,'rf__min_samples_split': [2, 10, 20]}\n","#grid = GridSearchCV(estimator=model_votation_majority, param_grid=params, cv=5)\n","#grid.best_params_\n","model_votation_majority = model_votation_majority.fit(PCA_lbp_vector_app, ytrain)\n","# Use the Combinaison model to predict the class of samples\n","Ytrain_pred_Votation = model_votation_majority.predict(PCA_lbp_vector_app)\n","Yvalid_pred_Votation = model_votation_majority.predict(PCA_lbp_vector_val)\n","Ytest_pred_Votation  = model_votation_majority.predict(PCA_lbp_vector_test)\n","\n","model_predict_ponderation_votation = model_predict_proba_votation.fit(PCA_lbp_vector_app, ytrain)\n","# Use the Combinaison model to predict the class of samples\n","Ytrain_pred_predict_proba = model_predict_ponderation_votation.predict(PCA_lbp_vector_app)\n","Yvalid_pred_predict_proba = model_predict_ponderation_votation.predict(PCA_lbp_vector_val)\n","Ytest_pred_predict_proba  = model_predict_ponderation_votation.predict(PCA_lbp_vector_test)\n","\n","dump(model_votation_majority, 'model_votation_majority_fit.joblib')\n","dump(model_predict_ponderation_votation, 'model_ponderation_votation.joblib')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['model_ponderation_votation.joblib']"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"markdown","metadata":{"id":"__bWRplN1RjP"},"source":["###  (2b): Résultats et résponses:"]},{"cell_type":"code","metadata":{"id":"GtTAqgqu1RjQ","executionInfo":{"status":"ok","timestamp":1587335513742,"user_tz":240,"elapsed":5925,"user":{"displayName":"Imane Zriaa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioDY4dLkpnQFW2LMOcW3EpM23co77w9n8Vs1QD=s64","userId":"18312604134454482575"}},"outputId":"b76817fe-f303-4c93-aa80-52f00cf50b0c","colab":{"base_uri":"https://localhost:8080/","height":134}},"source":["# Vos résultats ici:\n","scores_tr_ENSEMBLE  = accuracy_score(ytrain, Ytrain_pred_Votation )\n","scores_val_ENSEMBLE = accuracy_score(yval, Yvalid_pred_Votation )\n","scores_tst_ENSEMBLE = accuracy_score(ytest,  Ytest_pred_Votation )\n","print(\"Classification rate for the training dataset (Ensemble_MAJORITY)      = \"+str(scores_tr_ENSEMBLE*100)+\"%\")\n","print(\"Classification rate for the validation dataset  (Ensemble_MAJORITY)     = \"+str(scores_val_ENSEMBLE*100)+\"%\")\n","print(\"Classification rate for the test dataset (Ensemble_MAJORITY)      = \"+str(scores_tst_ENSEMBLE*100)+\"%\")\n","# Vos résultats ici:\n","# ENSEMBLE\n","scores_tr_ENSEMBLE  = accuracy_score(ytrain, Ytrain_pred_predict_proba )\n","scores_val_ENSEMBLE = accuracy_score(yval, Yvalid_pred_predict_proba )\n","scores_tst_ENSEMBLE = accuracy_score(ytest,  Ytest_pred_predict_proba )\n","print()\n","print(\"Classification rate for the training dataset (Ensemble_PREDICT_PROBA)      = \"+str(scores_tr_ENSEMBLE*100)+\"%\")\n","print(\"Classification rate for the validation dataset  (Ensemble_PREDICT_PROBA)     = \"+str(scores_val_ENSEMBLE*100)+\"%\")\n","print(\"Classification rate for the test dataset (Ensemble_PREDICT_PROBA)      = \"+str(scores_tst_ENSEMBLE*100)+\"%\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Classification rate for the training dataset (Ensemble_MAJORITY)      = 31.282872966665504%\n","Classification rate for the validation dataset  (Ensemble_MAJORITY)     = 22.596823627751466%\n","Classification rate for the test dataset (Ensemble_MAJORITY)      = 22.819726943438283%\n","\n","Classification rate for the training dataset (Ensemble_PREDICT_PROBA)      = 27.214462363718695%\n","Classification rate for the validation dataset  (Ensemble_PREDICT_PROBA)     = 24.463638896628588%\n","Classification rate for the test dataset (Ensemble_PREDICT_PROBA)      = 24.407913067706883%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ODB-BCLzyOeA","executionInfo":{"status":"ok","timestamp":1587352931875,"user_tz":240,"elapsed":3704,"user":{"displayName":"Imane Zriaa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioDY4dLkpnQFW2LMOcW3EpM23co77w9n8Vs1QD=s64","userId":"18312604134454482575"}},"outputId":"fff08367-8328-4c8e-ddec-e73970e964c6","colab":{"base_uri":"https://localhost:8080/","height":284}},"source":["# Fit the Ensemble (TRAIN)\n","# Sert seulement à l'évaluation les performances des classificateurs simples mis dans le VotingClassifier\n","clf1.fit(PCA_lbp_vector_app, ytrain)\n","scores_tr_ENSEMBLE  = accuracy_score(ytrain, clf1.predict(PCA_lbp_vector_app) )\n","scores_val_ENSEMBLE = accuracy_score(yval, clf1.predict(PCA_lbp_vector_val) )\n","scores_tst_ENSEMBLE = accuracy_score(ytest,  clf1.predict(PCA_lbp_vector_test) )\n","print()\n","print(\"Classification rate for the training dataset (DT)      = \"+str(scores_tr_ENSEMBLE*100)+\"%\")\n","print(\"Classification rate for the validation dataset  (DT)     = \"+str(scores_val_ENSEMBLE*100)+\"%\")\n","print(\"Classification rate for the test dataset (DT)      = \"+str(scores_tst_ENSEMBLE*100)+\"%\")\n","clf2.fit(PCA_lbp_vector_app, ytrain)\n","scores_tr_ENSEMBLE  = accuracy_score(ytrain, clf2.predict(PCA_lbp_vector_app) )\n","scores_val_ENSEMBLE = accuracy_score(yval, clf2.predict(PCA_lbp_vector_val) )\n","scores_tst_ENSEMBLE = accuracy_score(ytest,  clf2.predict(PCA_lbp_vector_test) )\n","print()\n","print(\"Classification rate for the training dataset (RT)      = \"+str(scores_tr_ENSEMBLE*100)+\"%\")\n","print(\"Classification rate for the validation dataset  (RT)     = \"+str(scores_val_ENSEMBLE*100)+\"%\")\n","print(\"Classification rate for the test dataset (RT)      = \"+str(scores_tst_ENSEMBLE*100)+\"%\")\n","clf3.fit(PCA_lbp_vector_app, ytrain)\n","scores_tr_ENSEMBLE  = accuracy_score(ytrain, clf3.predict(PCA_lbp_vector_app) )\n","scores_val_ENSEMBLE = accuracy_score(yval, clf3.predict(PCA_lbp_vector_val) )\n","scores_tst_ENSEMBLE = accuracy_score(ytest,  clf3.predict(PCA_lbp_vector_test) )\n","print()\n","print(\"Classification rate for the training dataset (NB)      = \"+str(scores_tr_ENSEMBLE*100)+\"%\")\n","print(\"Classification rate for the validation dataset  (NB)     = \"+str(scores_val_ENSEMBLE*100)+\"%\")\n","print(\"Classification rate for the test dataset (NB)      = \"+str(scores_tst_ENSEMBLE*100)+\"%\")\n","clf4.fit(PCA_lbp_vector_app, ytrain)\n","scores_tr_ENSEMBLE  = accuracy_score(ytrain, clf4.predict(PCA_lbp_vector_app) )\n","scores_val_ENSEMBLE = accuracy_score(yval, clf4.predict(PCA_lbp_vector_val) )\n","scores_tst_ENSEMBLE = accuracy_score(ytest,  clf4.predict(PCA_lbp_vector_test) )\n","print()\n","print(\"Classification rate for the training dataset (KNN)      = \"+str(scores_tr_ENSEMBLE*100)+\"%\")\n","print(\"Classification rate for the validation dataset  (KNN)     = \"+str(scores_val_ENSEMBLE*100)+\"%\")\n","print(\"Classification rate for the test dataset (KNN)      = \"+str(scores_tst_ENSEMBLE*100)+\"%\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Classification rate for the training dataset (DT)      = 25.845553659131284%\n","Classification rate for the validation dataset  (DT)     = 25.299526330454164%\n","Classification rate for the test dataset (DT)      = 25.13234884368905%\n","\n","Classification rate for the training dataset (RT)      = 44.99285938207531%\n","Classification rate for the validation dataset  (RT)     = 18.974644747840625%\n","Classification rate for the test dataset (RT)      = 20.78573418779604%\n","\n","Classification rate for the training dataset (NB)      = 25.277787453411822%\n","Classification rate for the validation dataset  (NB)     = 22.095291167456114%\n","Classification rate for the test dataset (NB)      = 22.48537196990805%\n","\n","Classification rate for the training dataset (KNN)      = 32.06659932425372%\n","Classification rate for the validation dataset  (KNN)     = 18.612426859849542%\n","Classification rate for the test dataset (KNN)      = 19.002507662301475%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kPH_0bA11RjT"},"source":["# Partie 3: Régroupement\n","## (FER dataset)\n","\n","\n","Dans cette partie, nous allons ignorer les tâches <font color=magenta>de classification ou régression</font> les et les étiquettes. Nous allons supposer que les données sont non supervisées (non étiquetées).\n","\n","Le but est de découvrir l’organisation des données, c.-à-d. découvrir les similitudes et les différences entre les données et d’en tirer des conclusions utiles (trouver des groupes cohérents).\n","\n","Les tâches de regroupement\n","<font color=magenta>\n","- Identifier des groupes « naturels » dans les données;\n","- Discretiser un espace $\\mathcal{R}^D$, en le séparant en $K$ régions. C'est equivalent à partitionner l’espace d’entrée en un certain nombre de « catégories » en se basant sur un ensemble d’apprentissage fini.\n","</font>\n"]},{"cell_type":"markdown","metadata":{"id":"mcUX8Ffm1RjT"},"source":["<font color=black>\n","\n","### Partie (3a): Code régroupement (FER):\n","    \n","##### À faire :\n","1. Utiliser les algorithmes de regroupement de [scikit-learn clustering](https://scikit-learn.org/stable/modules/clustering.html#clustering)\n","2. Choisir deux algorithmes de regroupement <br>\n","    <font color=magenta>Vous devez également regarder le tableau [Overview of clustering methods](https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods) pour voir quels sont les algorithmes de regroupement les plus appropriés en fonction des caractéristiques de l'ensemble de données FER (surtout « scalability » et « usecase »). (P.ex. $K$-means et hierarchical agglomerative. Voir le notebook exemple.)<br>\n","3. Entraîner et optimiser les paramètres des algorithmes de regroupement choisis. Utiliser l'ensemble de validation pour cette optimisation.<br>\n","4. Utiliser également l'ensemble de validation pour interpréter les résultats du regroupement. En regardant quelques exemples de chaque groupe, pouvons-nous assigner une étiquette fiable pour chaque groupe? P.ex. « Le groupe 0 représente l'émotion contente. Le groupe 1 représente l'émotion triste, etc. ».<br>\n","5. Faire une analyse des résultats et présenter vos conclusions sur l'interprétation des groupes trouvées. <br>\n"," - Analyser les groupes en supposant que les vraies étiquettes sont inconnues. <br>\n"," - Analyser les groupes en supposant que les vraies étiquettes sont connues.\n","     - Attention avec la consistance des étiquettes, c.-à-d., vous devez vous assurer d'avoir la bonne codification, chiffre = émotion.<br>\n","     - Dans le ground-truth nous avons 0 = Angry, 1 = Disgust, 2 = Fear, 3 = Happy, 4 = Sad, 5 = Surprise, 6 = Neutral, mais probablement vous n'allez pas avoir nécessairement cette même codification comme sortie d'algorithme de regroupement.\n","        \n","</font>    \n","</font>"]},{"cell_type":"code","metadata":{"id":"g3fb1Xfk1RjU"},"source":["image_shape = [48, 48]\n","\n","X_people = Xtrain\n","y_people = ytrain\n","X_people = X_people / 255"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_HKQ1ysmK3SY"},"source":["### PCA"]},{"cell_type":"code","metadata":{"id":"lNm7LMICK6cr"},"source":["def plot_pca(labels):\n","    X_pca_df = pd.DataFrame(data = X_pca)\n","    ## 2 first PCA components\n","    fig = plt.figure()\n","    fig.set_size_inches(20,10)\n","    plt.scatter(X_pca_df.iloc[:, 0], X_pca_df.iloc[:, 1],\n","                c = labels, edgecolor='none', alpha=0.5,\n","                cmap = plt.cm.get_cmap('Spectral', 7))\n","    plt.xlabel('component 1')\n","    plt.ylabel('component 2')\n","    plt.colorbar();\n","    \n","    from mpl_toolkits import mplot3d\n","\n","    ## 3 first PCA components\n","\n","    fig = plt.figure()\n","    fig.set_size_inches(20,10)\n","    ax = plt.axes(projection=\"3d\")\n","\n","    p = ax.scatter3D(X_pca_df.iloc[:, 0], X_pca_df.iloc[:, 1], X_pca_df.iloc[:, 2],\n","                     c = labels, cmap = 'Spectral')\n","\n","    ax.set_xlabel('component 1')\n","    ax.set_ylabel('component 2')\n","    ax.set_zlabel('component 3')\n","    fig.colorbar(p)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mtx-ExlWK77P"},"source":["pca = PCA( n_components = 100 )\n","pca.fit_transform( X_people )\n","X_pca = pca.transform( X_people )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zm8ayMYWLr0C"},"source":["#### Plot PCA"]},{"cell_type":"code","metadata":{"id":"Yu7vL8AOLtul"},"source":["plot_pca(y_people)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iTh6mAHdLxXc"},"source":["#### Performance Evaluation"]},{"cell_type":"code","metadata":{"id":"qXk5xdplLw4x"},"source":["def grouth_truth_eval(model_labels):\n","    # Perfect labeling is scored 1.0\n","    print(\"Adjusted Rand index: \", metrics.adjusted_rand_score(y_people, model_labels) )\n","\n","    # Perfect labeling is scored 1.0\n","    print(\"Mutual Information based scores: \", metrics.adjusted_mutual_info_score(y_people, model_labels) )\n","\n","    # Both are bounded below by 0.0 and above by 1.0 (higher is better)\n","    print(\"Homogeneity: \", metrics.homogeneity_score(y_people, model_labels) )\n","    print(\"Completeness: \", metrics.completeness_score(y_people, model_labels) )\n","    print(\"V-Measure: \", metrics.v_measure_score(y_people, model_labels) )\n","    print(\"Fowlkes-Mallows scores: \", metrics.fowlkes_mallows_score(y_people, model_labels) )\n","\n","def non_grouth_eval(model_labels):\n","    # The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters.\n","    # The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.\n","    print(\"Silhouette coefficient: \", metrics.silhouette_score(X_pca, model_labels , metric='euclidean') )\n","\n","    # The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.\n","    print(\"Calinski-Harabasz index: \", metrics.calinski_harabasz_score(X_pca, model_labels) )\n","\n","    # Zero is the lowest possible score. Values closer to zero indicate a better partition.\n","    print(\"Davies-Bouldin Index: \", metrics.davies_bouldin_score(X_pca, model_labels) )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZQTC7SdFMA4r"},"source":["### BIRCH\n"]},{"cell_type":"code","metadata":{"id":"tgngtNP59nBZ"},"source":["birch = Birch(threshold=0.005, n_clusters=7, compute_labels=True, copy=True)\n","\n","clusters = birch.fit_predict(X_pca)\n","            \n","print(\"cluster sizes Birch: %s\" % np.bincount(clusters))\n","\n","core_samples_mask = np.zeros_like(birch.labels_)\n","labels = birch.labels_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-_e6B7qG9yxz"},"source":["# Number of clusters in labels, ignoring noise if present.\n","n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n","n_noise_ = list(labels).count(-1)\n","n_clusters_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q0vANpKt-fW0"},"source":["# Utilise pour comparer avec les vrais etiquettes\n","plot_pca(labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tmtcxa59-hvp"},"source":["clusters = birch.fit_predict(X_pca)\n","grouth_truth_eval(clusters, y_people)\n","non_grouth_eval(clusters, X_pca)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HS3tRzfi-h37"},"source":["clusters = birch.predict(X_val_pca)\n","grouth_truth_eval(clusters, yval)\n","non_grouth_eval(clusters, X_val_pca)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MBYrGVfc-d_h"},"source":["clusters = birch.fit_predict(X_test_pca)\n","grouth_truth_eval(clusters, ytest)\n","non_grouth_eval(clusters, X_test_pca)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z7ZIEMcA-kh2"},"source":["#### Optimisation\n"]},{"cell_type":"code","metadata":{"id":"1CtCIDQD-nnw"},"source":["for n_cluster in range(2, 11):\n","    spec = Birch(n_clusters=n_cluster, threshold=0.005).fit(X_val_pca)\n","    label = spec.labels_\n","    sil_coeff = silhouette_score(X_val_pca, label, metric='euclidean')\n","    print(\"For n_clusters={}, The Silhouette Coefficient is {}\".format(n_cluster, sil_coeff))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VSH2V1h4-nvL"},"source":["for n_cluster in range(2, 6):\n","    spec = Birch(n_clusters=n_cluster, threshold=0.005, branching_factor=10*n_cluster).fit(X_val_pca)\n","    label = spec.labels_\n","    sil_coeff = silhouette_score(X_val_pca, label, metric='euclidean')\n","    print(\"For branching factor={}, The Silhouette Coefficient is {}\".format(10*n_cluster, sil_coeff))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1UZs3VvnMPiC"},"source":["### DBSCAN\n"]},{"cell_type":"code","metadata":{"id":"QNMkwLV8Q8I0"},"source":["# covariance = np.cov(X_pca.astype(\"float32\"), rowvar=False)\n","# db = DBSCAN(eps=0.1, min_samples=15, algorithm='ball_tree')\n","# db = DBSCAN(eps=0.5, min_samples=5, algorithm='ball_tree', metric='minkowski', leaf_size=90, p=2)\n","# db = DBSCAN(min_samples=6, eps=3, metric=\"mahalanobis\", metric_params={\"V\": covariance})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4qjuf2mjQzrH"},"source":["# Compute DBSCAN\n","db = DBSCAN(eps=0.03, min_samples=10, algorithm='ball_tree', metric='minkowski', leaf_size=90, p=2)\n","clusters = db.fit_predict(X_pca)\n","\n","core_samples_mask = np.zeros_like(db.labels_)\n","core_samples_mask[db.core_sample_indices_] = True\n","labels = db.labels_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EjpKreNLQz6e"},"source":["# Number of clusters in labels, ignoring noise if present.\n","n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n","n_noise_ = list(labels).count(-1)\n","n_clusters_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BEdk8mLfQz_u"},"source":["grouth_truth_eval(clusters, y_people)\n","non_grouth_eval(clusters, X_pca)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wyFzccLnQz3k"},"source":["clusters = db.fit_predict(X_val_pca)\n","grouth_truth_eval(clusters, ytest)\n","non_grouth_eval(clusters, X_val_pca)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oDZXN3VFQz08"},"source":["clusters = db.fit_predict(X_test_pca)\n","grouth_truth_eval(clusters, ytest)\n","non_grouth_eval(clusters, X_test_pca)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xLxu5rUcQzyc"},"source":["plot_pca(labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q4fo5_vbRD8m"},"source":["#### Optimisation\n"]},{"cell_type":"code","metadata":{"id":"EYR4tIq5RGAO"},"source":["# for eps_op in [0.01, 0.02, 0.03, 0.05]:\n","#     spec = DBSCAN(eps=eps_op, min_samples=10).fit(X_val_pca)\n","#     label = spec.labels_\n","#     sil_coeff = silhouette_score(X_val_pca, label, metric='euclidean')\n","#     print(\"For eps={}, Epsilan is {}\".format(eps_op, sil_coeff))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XBID7QWVR4D7"},"source":["# for min_op in [5, 10, 15,  20, 30, 50]:\n","#     spec = DBSCAN(eps=0.03, min_samples=min_op).fit(X_val_pca)\n","#     label = spec.labels_\n","#     sil_coeff = silhouette_score(X_val_pca, label, metric='euclidean')\n","#     print(\"For min_sample={}, min sample is {}\".format(min_op, sil_coeff))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R0QNWyuKZuga"},"source":["Ces deux cellules ne fonctionnent pas, mais ils sont présent à titre indicative pour montrer quels valeurs à optimiser. "]},{"cell_type":"markdown","metadata":{"id":"E4o_5Uci-4i7"},"source":["### Analyse \n"]},{"cell_type":"markdown","metadata":{"id":"VR4fAIB01RjW"},"source":["### Partie (3a): Résultats et résponses:"]},{"cell_type":"markdown","metadata":{"id":"UwtXJc8h1RjX"},"source":["#### Votre analyse et résultats ici:\n","\n","<font color=black>\n","    \n","##### Birch (évaluation sans étiquettes):\n","| Ensemble | AR index | MI score | Homo | Completeness | V-measure | F-M score |\n","|----------|----------|----------|------|--------------|-----------|-----------|\n","| App      | 0.008     | 0.012     |0.014 | 0.014   | 0.014    | 0.184 |\n","| Val      | 0.013     | 0.012    | 0.014 | 0.0178  |  0.016    | 0.217 |\n","| Test     | 0.012    | 0.014     | 0.014 |  0.017     |   0.0178    | 0.0159 |\n","\n","\n","##### Birch (évaluation avec étiquettes):\n","| Ensemble | Sillouette | C-H score | D-B index |\n","|----------|------------|-----------|-----------|\n","| App      |  0.378  | 2639.364     | 2.758 |\n","| Val      |  0.053  | 320.110     | 2.95 |\n","| Test     | 0.0625   | 358.59     | 2.5974 |\n","\n","\n","</font>    "]},{"cell_type":"markdown","metadata":{"id":"tukYfWPFKDxV"},"source":[" Pour que les algorithmes de regroupement fonctionnent correctement, il nous faut \n","avant tout des groupes de données discernables. En entrée, nous avons les données FER où nous avons implémenté PCA à 100 composants pour diminuer la taille de données des vecteurs. Lorsqu'on trace en x,y et z, les trois premiers composants PCAs avec les \n","étiquettes réelles. Il est possible de voir que toutes les émotions sont confondues. Il est impossible de distinguer des zones de concentrations d'émotions distinctes.Si nous regardons  les clusters créés par notre algorithme, il est possible de voir que la zone\n","-0.1 à -0.025 de l'axe du composant 1, il y a de l'émotion 0, vers la zone 0.02 à 0.06, il a plus de l'émotion 2 et 3, etc. Cette division claire des émotions est issue de l'algorithme et ne  reflète pas la réalité comme il est possible de le voir par notre évaluation avec étiquettes.En effet, par exemple, notre ajusted rand index de 0.012, alors qu'un résultat parfait serait 1.  Pour notre évaluation sans étiquette, nous remarquons que nos clusters ne sont bien définis(silhouette coefficient). Les clusters sont très denses. Cela est normal, car notre threshold est très petit. Et notre D-B index n'est pas à zéro, mais elle est proche. Cela indique une bonne partition des clusters. \n","\n","Pour optimiser notre algorthme Birch, nous avons fait varier le nombre de clusters et notre branching factor. Nous avons gardé un threshold bas de 0.005, car nos points sont très proches et un threshold élevé cause des erreurs potentielles avec créant pas assez de clusters. \n","Dans chacun des cas, nous voulons trouver un coefficient de silhouette élevé. Selon la documentation de sklearn, un coefficient élevé nous dit que notre modèle possède des clusters mieux définis. Pour le nombre de clusters, 2 clusters nous donnent le mieux coefficient. Or étant donné que nous avons sept classes, deux clusters ne sont simplement pas suffisants. Le nombre de clusters qui est mieux est 5, avec un coefficient de 0.083. Ces résultats ne sont certainement pas élevés, mais c'est le mieux que nous avions pû trouvé. Pour le branching factor, nous avons trouvé que 20 est le meilleur choix. Un branching factor plus faible (le nombre par défaut est de 50)nous permet d'avoir une meilleure séparation des donnés."]},{"cell_type":"markdown","metadata":{"id":"GbhYemtrKgwF"},"source":["\n","##### DBSCAN (évaluation sans étiquettes):\n","| Ensemble | AR index | MI score | Homo | Completeness | V-measure | F-M score |\n","|----------|----------|----------|------|--------------|-----------|-----------|\n","| App      | 0.006     | 0.006     |0.005 | 0.014   | 0.007    | 0.32 |\n","| Val      | X.XXX     | X.XXX    | X.XXX | X.XXX           |  X.XXX    | X.XXX |\n","| Test     | X.XXX    | X.XXX     | X.XXX |  X.XXX     |   X.XXX    | X.XXX |\n","\n","\n","##### DBSCAN (évaluation avec étiquettes):\n","| Ensemble | Sillouette | C-H score | D-B index |\n","|----------|------------|-----------|-----------|\n","| App      |  -0.12  | 22.312     | 3.748 |\n","| Val      |  X.XXX  | X.XXX     | X.XXX |\n","| Test     | X.XXX   | X.XXX     | X.XXX |"]},{"cell_type":"markdown","metadata":{"id":"X5jUmyanKhBM"},"source":["Pour DBSCAN, il y a deux paramètres à faire varier pour changer le nombre et dimension des clusters produites: epsilon et min_samples. Après de nombreux essais, nous n'avons pas trouvés de modèle adéquat pour regrouper nos dataset val et test pour FER. En effet, en diminuant notre epsilon et notre min_sample,nous voulons créer plusieurs clusters. Or en ce faisant, nous avions des dizaines voire des centaines de clusters, mais seulement un cluster pour nos données val et test. Lorsqu'on trace les étiquettes trouvées par notre modèle, on remarque une surabondance d'émotion 0 avec les autres émotions quasiment non existantes.Cet état n'est probablement pas dû à DBSCAN, mais à notre inhabileté à faire marcher l'algorithme à dataset.\n","Nous avons choisi DBSCAN après avoir lu cet article (http://fse.studenttheses.ub.rug.nl/18064/1/Report_research_internship.pdf) qui met DBSCAN et Mean Shift dans une lumière favorable pour le clustering de visages similaires. "]},{"cell_type":"markdown","metadata":{"id":"0SC3YOnr1RjX"},"source":["# Partie Final: Conclusion"]},{"cell_type":"markdown","metadata":{"id":"x6i4i01x1RjY"},"source":["##### À faire (SVM et SVR):\n","\n","#\n","<font color=magenta>\n","1. Décrivez la méthode de normalisation de données utilisée.<br>\n","</font>\n"," Nous utilisons MinMaxScaler de la librairie scikit learn pour faire la normalisation de données entre des valeurs de 0 et 1, Cet classe met à l'échelle et traduit chaque caractéristique individuellement de telle sorte qu'elle se trouve dans la plage donnée sur l'ensemble de données choisis.\n","\n","#\n","<font color=magenta>\n","2. Présentez brièvement la méthode que vous avez utilisée afin de trouver le meilleur modèle SVM. Quels ont été vos résultats ? Quels sont les impacts des hyperparamètres et leur utilité respective?<br>\n","</font>\n","Nous avons choisi d'y aller avec le GridSearch de la librairie hypopt pour faire des tests avec les différents hyperparamètres dans les modèles SVM, même si cela à pris du temps à faire les calculs, ce sont les paramètres qui permettent d'améliorer la précision des modèles.\n","\n","#\n","<font color=magenta>\n","3. Globalement, prenant en compte les trois TPs, quel type d’approche recommanderiez-vous pour les ensembles de données FER et FG-NET et dans quelles conditions (par exemple, mais non exhaustif, le nombre de données privilégié, les hyperparamètres, le temps de calcul, le matériel nécessaire, les scores de performance)? Discutez des performances que vous avez obtenues entre tous les modèles d’apprentissage. <br>\n","</font>\n","\n","À travers ces trois TPs, le modèle qui a le mieux performé est notre CNN à sept couches cachées. En effet, avec le data augmentation, nous avions une précision d'un peu de 70% pour FER et FG-NET. Cela est nettement supérieur à toutes les autres modèles vues dans nos laboratoires.  Pour l'entrainer, il nous fallait peu de prétraitement des données et aucune extraction de primitives. Les seuls facteurs limitants sont le matériel nécessaire pour entrainer ce modèle. En effet, c'est seulement grâce aux GPUs de l'école que nous avons pu aller à sept couches. Or, en tenant compte des résultats obtenus. Il est clair que le CNN est le meilleur choix.\n","\n","Cela étant dit, nous allons parler des modèles les plus pertinents de chaque laboratoire. Dans le laboratoire 1, avec notre meilleur modèle KNN, nous avons eu une précision de 27% avec qui est très bas relative à notre CNN. Cette précision a été trouvée après avoir effectué un gridSearch pour trouver les meilleurs hyperparametres. Nous avions aussi extrait des primitives avec SIFT et LBP. Ces vecteurs extraits ne fournissaient pas des primitives discriminantes, comme il est possible de voir avec les graphiques tracés avec ces derniers. Cela a contribué à faible performance. \n","\n"," Dans le laboratoire 2, nous avons travaillé avec des algorithmes de régression linéaires. La régression logistique du dataset FER a produit la précision la plus élevée avec 36%. Lorsqu'on a utilisé les vecteurs SIFT et LBP, cette précision à baissé de 10%. Cela prouve encore une fois que les vecteurs extraits ne sont pas discriminants. Ensuite nous avons testé un MLP et encore une fois en utilisant le dataset FER au complet, nous avons une meilleure précision qu'en utilisant les vecteurs de primitives; 29% v.s 15 à 17%. Le meilleur résultat avec les vecteurs de primitives extraites viennent de PCA LBP avec 24.49%.\n","\n","\n","\n","#\n","<font color=magenta>\n","4. Formulez quelques pistes d’amélioration des classificateurs.<br>\n","</font>\n","\n","Pour améliorer la plupart des modèles, il faut trouver des primitives discriminantes. Comme il est possible de voir avec toutes les autres modèles dans les laboratoires précédentes, la performance des modèles dépendent énormément de l'information transmise, soit l'adage Garbage In, Garbage out. Si nous avions pu trouver dans le laboratoire 1 des primitives discriminantes de qualité, la performance de toutes les modèles auraient pû être améliorée.  \n","    \n","##### À faire (Combinaison de modèles):\n","#\n","<font color=magenta>\n","1. Présentez la conception de votre solution reposant sur la théorie des ensembles. Présentez ici le diagramme nécessaire afin de présenter convenablement votre combinaison de modèles. Faites une discussion expliquant vos décisions de conception.<br>\n","</font>\n","Utilisation du VotingClassifier de la librairie scikit-learn pour faire la combinaison de modèles selon deux différentes règles soit __le vote par majorité__ et __la votation pondérée sur les classes prédites__, le jeu de primitives PCA_lbp est le vecteur de données pour faire l'évaluation de ce classificateur par votation avec quatre différents modèles de classification, soit le DecisionTreeClassifier, le randomForestClassifier, le gaussian naives bayes et KNeighborsClassifier.\n","\n","|JEU DE PRIMITIVES| CLASSIFICATEUR |COMBINAISON |RÉSULTATS|\n","|-|-|-|-|                   \n","||                   ---->DT------>\n","||                   ---->RT------>\n","|PCA_lbp -------->|================|--->VotingClassifier--->|y\n","||                   ---->NB------>\n","||                   ---->nKK----->\n","\n","|JEU DE PRIMITIVES| CLASSIFICATEUR |COMBINAISON |RÉSULTATS|\n","|-|-|-|-|                   \n","||                   ---->DT------>(1.8)\n","||                   ---->RT------>(0.8)\n","|PCA_lbp -------->|================|--->VotingClassifier--->|y\n","||                   ---->NB------>(1.2)\n","||                   ---->nKK----->(0.8)\n","\n","\n","#\n","<font color=magenta>\n","2. Est-ce que la combinaison de modèles a apporté des améliorations dans les performances? Comparer la combinaison de modèles avec les modèles individuels.<br>\n","</font>\n","La performance par les six classificateurs individuels de la partie 1 sont moins élevé avec des précisions obtenues entre 16% et 18% pour l'ensemble validation et l'ensemble test, tandis que les ensembles à l'aide de VotingClassifier obtiennent au moins une précision de 22% et 24% pour les ensembles de validation et de test, ce qui représente une différence d'au moins 4% entre les classificateurs simples et des ensembles avec VotingClassifier. \n","\n","    \n","\n","##### À faire (Regroupement): \n","<font color=magenta>    \n","1. Présentez vos conclusions sur les algorithmes de regroupement. Sont-ils vraiment utiles? Ont-ils produit des résultats cohérents? Est-ce que c’est facile à analyser les sorties (groupes) et d'en tirer des conclusions sur quelles étiquettes assigner à ces groupes?<br>\n","</font>\n","\n","    \n","    "]},{"cell_type":"markdown","metadata":{"id":"jrigyEORQcJF"},"source":["Théoriquement, les émotions et leur caractéristiques sont distinctes. Dans cette mesure, il doit être possible de créer des clusters capable de diviser l'informations en clusters. Cependant, avec notre dataset FER, il n'est possible de dire que les algorithmes de regroupement sont utiles.  Les données ne semblent pas être capables d'être regroupées en clusters alors les deux algorithmes utilisés ne fonctionnent pas. Les clusters créés ne représentent pas la réalité des données fournies alors les conclusions tirées seront forcément erronées. "]}]}